% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/clean.R
\name{smash_tokenize}
\alias{smash_tokenize}
\title{Tokenize data frame}
\usage{
smash_tokenize(data_frame, min_n = 30, custom_stopwords = NULL, ...)
}
\arguments{
\item{data_frame}{a data frame like the one produced by the smash_clean() function}

\item{min_n}{a number, minimum threshold for words}

\item{custom_stopwords}{a character vector of stopwords (optional)}

\item{...}{extra arguments to internal function break_into_sentences(), (e.g. \code{remove_sentences = c("show more", "continue reading", ".")}, \code{parallel = FALSE})}
}
\value{
a data frame of tokens
}
\description{
Tokenize data frame
}
